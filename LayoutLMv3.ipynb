{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.14",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "sourceId": 2916806,
          "sourceType": "datasetVersion",
          "datasetId": 1569295
        }
      ],
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "import kagglehub\n",
        "urbikn_sroie_datasetv2_path = kagglehub.dataset_download('urbikn/sroie-datasetv2')\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "34KlyzwCdoW3"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LayoutLMv3-Invoice Extract: Fine-Tuning for Invoice Understanding\n",
        "\n",
        "**Project Idea:**  \n",
        "This project focuses on fine-tuning the **LayoutLMv3** model on the SERIO dataset to improve invoice understanding and entity extraction. The primary goal is to enhance the model's ability to accurately interpret complex invoice layouts, identifying key information such as invoice numbers, dates, total amounts, and line items. By leveraging the LayoutLMv3 model's visual and textual learning capabilities, the project aims to achieve more efficient and accurate processing of invoices for real-world applications in financial management and automated data extraction.\n",
        "\n",
        "**Objectives:**\n",
        "- Fine-tune LayoutLMv3 on the SERIO dataset for better invoice understanding.\n",
        "- Improve the model's ability to recognize key entities such as invoice numbers, dates, total amounts, and line items.\n",
        "- Apply the model in real-world scenarios to automate invoice data extraction and financial management.\n",
        "\n",
        "**Conclusion:**  \n",
        "By enhancing the LayoutLMv3 model's ability to accurately interpret invoices, this project seeks to advance automated solutions for financial document processing, leading to more streamlined workflows in financial and administrative tasks.\n"
      ],
      "metadata": {
        "id": "zGV64wJPdoW4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Required Libraries and Imports\n",
        "\n",
        "The following libraries and modules are essential for data processing, model training, evaluation, and visualization. They include utilities for handling datasets, image transformations, tokenization, and metrics calculation. The LayoutLMv3 model and Trainer from Hugging Faceâ€™s `transformers` library are also imported to facilitate token classification tasks on structured documents like invoices.\n",
        "\n",
        "- **os, glob, shutil**: For file handling and directory management.\n",
        "- **PIL (Python Imaging Library)**: For image processing and rendering.\n",
        "- **cv2 (OpenCV)**: For advanced image manipulation and visualization.\n",
        "- **torch, torchvision**: For building and training deep learning models.\n",
        "- **transformers**: To use the LayoutLMv3 model, tokenizers, and the Trainer class.\n",
        "- **sklearn**: To calculate metrics such as accuracy, precision, recall, and F1 score.\n",
        "- **matplotlib**: For visualizing images and bounding boxes.\n",
        "- **tqdm**: To display progress bars during training."
      ],
      "metadata": {
        "id": "BJZzqKI7doW5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "import json\n",
        "import random\n",
        "from pathlib import Path\n",
        "from difflib import SequenceMatcher\n",
        "import shutil\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "import cv2\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.notebook import tqdm\n",
        "from IPython.display import display\n",
        "import matplotlib\n",
        "from matplotlib import pyplot, patches\n",
        "from time import perf_counter\n",
        "import random\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "from transformers import LayoutLMv3Tokenizer\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from transformers import TrainingArguments, Trainer\n",
        "from transformers import LayoutLMv3ForTokenClassification, AutoConfig\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "import numpy as np\n",
        "\n"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "id": "jrQLxBSbdoW6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### let's see one invoice to gain some insights"
      ],
      "metadata": {
        "id": "XiQwQyq4doW6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sroie_folder_path = Path('/kaggle/input/sroie-datasetv2/SROIE2019')\n",
        "example_file = Path('X51005365187.txt')"
      ],
      "metadata": {
        "trusted": true,
        "id": "XuzsrXaUdoW6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image = Image.open(\"/kaggle/input/sroie-datasetv2/SROIE2019/train/img/X00016469612.jpg\")\n",
        "image = image.convert(\"RGB\")\n",
        "new_image = image.resize((300, 600))\n",
        "new_image\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "kJC-8PqsdoW6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Prepocessing"
      ],
      "metadata": {
        "id": "MPT5iUbudoW6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reading Bounding Boxes and Words from Text Files\n",
        "\n",
        "This function `read_bbox_and_words` reads bounding box coordinates and text from a given file and processes the data into a structured format. The text file is expected to contain comma-separated values where the first eight values represent the bounding box coordinates (in terms of four points: x0, y0, x1, y1, x2, y2, x3, y3) and the remaining values correspond to the associated text. The function splits these values, stores them in a list, and finally converts them into a Pandas DataFrame.\n",
        "\n",
        "### Steps:\n",
        "1. **File Reading and Parsing**: The function opens the specified file, processes each line, and extracts bounding box coordinates along with the corresponding text.\n",
        "2. **Data Storage**: The parsed data is stored in a list and then converted into a DataFrame for easier handling and manipulation.\n",
        "3. **Bounding Box Conversion**: The bounding box coordinates are explicitly converted into integers for future processing.\n",
        "4. **Dropping Unnecessary Columns**: Some of the bounding box columns are dropped to simplify the data (e.g., `x1`, `y1`, `x3`, and `y3` are removed).\n",
        "5. **Preview**: The function returns the processed DataFrame, and we display the first few rows of the file and the DataFrame to verify the result.\n",
        "\n",
        "The `head()` command is used to show the first five lines from the input file and the first few rows of the resulting DataFrame.\n"
      ],
      "metadata": {
        "id": "kv52PjmYdoW6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def read_bbox_and_words(path: Path):\n",
        "  bbox_and_words_list = []\n",
        "\n",
        "  with open(path, 'r', errors='ignore') as f:\n",
        "    for line in f.read().splitlines():\n",
        "      if len(line) == 0:\n",
        "        continue\n",
        "\n",
        "      split_lines = line.split(\",\")\n",
        "\n",
        "      bbox = np.array(split_lines[0:8], dtype=np.int32)\n",
        "      text = \",\".join(split_lines[8:])\n",
        "\n",
        "      # From the splited line we save (filename, [bounding box points], text line).\n",
        "      # The filename will be useful in the future\n",
        "      bbox_and_words_list.append([path.stem, *bbox, text])\n",
        "\n",
        "  dataframe = pd.DataFrame(bbox_and_words_list, columns=['filename', 'x0', 'y0',\n",
        "                                                         'x1', 'y1', 'x2', 'y2', 'x3', 'y3', 'line'])\n",
        "\n",
        "  # Explicitly convert only the bounding box columns to integers\n",
        "  bbox_columns = ['x0', 'y0','x1', 'y1', 'x2', 'y2', 'x3', 'y3']  # Adjust based on your actual columns\n",
        "  dataframe[bbox_columns] = dataframe[bbox_columns].astype(np.int16)\n",
        "\n",
        "  dataframe = dataframe.drop(columns=['x1', 'y1', 'x3', 'y3'])\n",
        "\n",
        "  return dataframe\n",
        "bbox_file_path = sroie_folder_path / \"test/box\" / example_file\n",
        "print(\"== File content ==\")\n",
        "!head -n 5 \"{bbox_file_path}\"\n",
        "\n",
        "bbox = read_bbox_and_words(path=bbox_file_path)\n",
        "print(\"\\n== Dataframe ==\")\n",
        "bbox.head(5)"
      ],
      "metadata": {
        "trusted": true,
        "id": "a9RWzhM4doW6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Read Entities from JSON:\n",
        "This function reads invoice entities from a JSON file and returns them as a Pandas DataFrame.\n"
      ],
      "metadata": {
        "id": "mkvdDaxpdoW7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def read_entities(path: Path):\n",
        "  with open(path, 'r') as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "  dataframe = pd.DataFrame([data])\n",
        "  return dataframe\n",
        "\n",
        "\n",
        "# Example usage\n",
        "entities_file_path = sroie_folder_path /  \"test/entities\" / example_file\n",
        "print(\"== File content ==\")\n",
        "!head \"{entities_file_path}\"\n",
        "\n",
        "entities = read_entities(path=entities_file_path)\n",
        "print(\"\\n\\n== Dataframe ==\")\n",
        "entities"
      ],
      "metadata": {
        "trusted": true,
        "id": "wA-O4w1RdoW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Assign Line Label:\n",
        "This function assigns a label to a line of text based on its similarity to entity names from a DataFrame, returning the matching entity type or \"O\" for no match.\n"
      ],
      "metadata": {
        "id": "V7hWYxwwdoW7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def assign_line_label(line: str, entities: pd.DataFrame):\n",
        "    line_set = line.replace(\",\", \"\").strip().split()\n",
        "    for i, column in enumerate(entities):\n",
        "        entity_set =  entities.iloc[0, i].replace(\",\", \"\").strip().split()\n",
        "\n",
        "\n",
        "        matches_count = 0\n",
        "        for l in line_set:\n",
        "            if any(SequenceMatcher(a=l, b=b).ratio() > 0.8 for b in entity_set):\n",
        "                matches_count += 1\n",
        "\n",
        "            if (column.upper() == 'ADDRESS' and (matches_count / len(line_set)) >= 0.5) or \\\n",
        "               matches_count == len(entity_set):\n",
        "                return column.upper()\n",
        "\n",
        "    return \"O\"\n",
        "\n",
        "\n",
        "line = bbox.loc[1,\"line\"]\n",
        "label = assign_line_label(line, entities)\n",
        "print(\"Line:\", line)\n",
        "print(\"Assigned label:\", label)"
      ],
      "metadata": {
        "trusted": true,
        "id": "6lAsFyz2doW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Assign Labels:\n",
        "This function assigns labels to words based on their bounding box dimensions and the presence of entities, ensuring unique assignments for critical fields like TOTAL and DATE while preventing conflicts.\n"
      ],
      "metadata": {
        "id": "eN6kvsUcdoW7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def assign_labels(words: pd.DataFrame, entities: pd.DataFrame):\n",
        "    max_area = {\"TOTAL\": (0, -1), \"DATE\": (0, -1)}  # Value, index\n",
        "    already_labeled = {\"TOTAL\": False,\n",
        "                       \"DATE\": False,\n",
        "                       \"ADDRESS\": False,\n",
        "                       \"COMPANY\": False,\n",
        "                       \"O\": False\n",
        "    }\n",
        "\n",
        "    # Go through every line in $words and assign it a label\n",
        "    labels = []\n",
        "    for i, line in enumerate(words['line']):\n",
        "        label = assign_line_label(line, entities)\n",
        "\n",
        "        already_labeled[label] = True\n",
        "        if (label == \"ADDRESS\" and already_labeled[\"TOTAL\"]) or \\\n",
        "           (label == \"COMPANY\" and (already_labeled[\"DATE\"] or already_labeled[\"TOTAL\"])):\n",
        "            label = \"O\"\n",
        "         # Assign to the largest bounding box\n",
        "        if label in [\"TOTAL\", \"DATE\"]:\n",
        "            x0_loc = words.columns.get_loc(\"x0\")\n",
        "            bbox = words.iloc[i, x0_loc:x0_loc+4].to_list()\n",
        "            area = (bbox[2] - bbox[0]) + (bbox[3] - bbox[1])\n",
        "\n",
        "            if max_area[label][0] < area:\n",
        "                max_area[label] = (area, i)\n",
        "\n",
        "            label = \"O\"\n",
        "\n",
        "        labels.append(label)\n",
        "\n",
        "    labels[max_area[\"DATE\"][1]] = \"DATE\"\n",
        "    labels[max_area[\"TOTAL\"][1]] = \"TOTAL\"\n",
        "\n",
        "    words[\"label\"] = labels\n",
        "    return words\n",
        "\n",
        "\n",
        "# Example usage\n",
        "bbox_labeled = assign_labels(bbox, entities)\n",
        "bbox_labeled.head(15)"
      ],
      "metadata": {
        "trusted": true,
        "id": "CZOBaogfdoW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bbox_labeled.columns"
      ],
      "metadata": {
        "trusted": true,
        "id": "bs9lXjAwdoW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Split Line:\n",
        "This function splits a line into individual words while maintaining the same bounding box coordinates for each word, as the research indicates that they share the same context.\n"
      ],
      "metadata": {
        "id": "ka3NW2BhdoW7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def split_line(line: pd.Series) -> list:\n",
        "    \"\"\"\n",
        "    Splits a line into words and updates bounding box coordinates for each word.\n",
        "\n",
        "    Parameters:\n",
        "        line (pd.Series): A pandas Series with 'x0', 'x2', and 'line' columns.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of lists where each sublist contains updated values for the line.\n",
        "    \"\"\"\n",
        "    # Ensure the line has the necessary columns\n",
        "    if not {'x0', 'x2', 'line'}.issubset(line.index):\n",
        "        raise ValueError(\"The line must contain 'x0', 'x2', and 'line' columns.\")\n",
        "\n",
        "    # Extract current bounding box information\n",
        "    x0 = line['x0']\n",
        "    x2 = line['x2']\n",
        "    bbox_width = line['x2'] - line['x0']  # Example width, adjust as needed\n",
        "    line_str = line['line']\n",
        "\n",
        "    words = line_str.split()\n",
        "    new_lines = []\n",
        "\n",
        "    # Iterate through each word and calculate new bounding box coordinates\n",
        "    for index, word in enumerate(words):\n",
        "\n",
        "        # Create a new Series for the updated line\n",
        "        line_copy = line.copy()\n",
        "        line_copy['x0'] = x0\n",
        "        line_copy['x2'] = x2\n",
        "        line_copy['line'] = word\n",
        "\n",
        "        # Append the updated line to the new_lines list\n",
        "        new_lines.append(line_copy.to_list())\n",
        "\n",
        "        # Update x0 for the next word\n",
        "    return new_lines\n",
        "\n",
        "\n",
        "\n",
        "# Example usage\n",
        "new_lines = split_line(bbox_labeled.loc[1])\n",
        "print(\"Original row:\")\n",
        "display(bbox_labeled.loc[1:1,:])\n",
        "\n",
        "print(\"Splitted row:\")\n",
        "pd.DataFrame(new_lines, columns=bbox_labeled.columns)"
      ],
      "metadata": {
        "trusted": true,
        "id": "WCALdl6VdoW8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def dataset_creator(folder: Path):\n",
        "  bbox_folder = folder / 'box'\n",
        "  entities_folder = folder / 'entities'\n",
        "  img_folder = folder / 'img'\n",
        "\n",
        "  # Sort by filename so that when zipping them together\n",
        "  # we don't get some other file (just in case)\n",
        "  entities_files = sorted(entities_folder.glob(\"*.txt\"))\n",
        "  bbox_files = sorted(bbox_folder.glob(\"*.txt\"))\n",
        "  img_files = sorted(img_folder.glob(\"*.jpg\"))\n",
        "\n",
        "  data = []\n",
        "\n",
        "  print(\"Reading dataset:\")\n",
        "  for bbox_file, entities_file, img_file in tqdm(zip(bbox_files, entities_files, img_files), total=len(bbox_files)):\n",
        "    # Read the files\n",
        "    bbox = read_bbox_and_words(bbox_file)\n",
        "    entities = read_entities(entities_file)\n",
        "    image = Image.open(img_file)\n",
        "\n",
        "    # Assign labels to lines in bbox using entities\n",
        "    bbox_labeled = assign_labels(bbox, entities)\n",
        "    del bbox\n",
        "\n",
        "    # Split lines into separate tokens\n",
        "    new_bbox_l = []\n",
        "    for index, row in bbox_labeled.iterrows():\n",
        "      new_bbox_l += split_line(row)\n",
        "    new_bbox = pd.DataFrame(new_bbox_l, columns=bbox_labeled.columns)\n",
        "    new_bbox[['x0', 'y0', 'x2', 'y2']] = new_bbox[['x0', 'y0', 'x2', 'y2']].astype(np.int16)\n",
        "\n",
        "    del bbox_labeled\n",
        "\n",
        "\n",
        "    # Do another label assignment to keep the labeling more precise\n",
        "    for index, row in new_bbox.iterrows():\n",
        "      label = row['label']\n",
        "\n",
        "      if label != \"O\":\n",
        "        entity_values = entities.iloc[0, entities.columns.get_loc(label.lower())]\n",
        "        entity_set = entity_values.split()\n",
        "\n",
        "        if any(SequenceMatcher(a=row['line'], b=b).ratio() > 0.7 for b in entity_set):\n",
        "            label = \"S-\" + label\n",
        "        else:\n",
        "            label = \"O\"\n",
        "\n",
        "      new_bbox.at[index, 'label'] = label\n",
        "\n",
        "    width, height = image.size\n",
        "\n",
        "    data.append([new_bbox, width, height])\n",
        "\n",
        "  return data"
      ],
      "metadata": {
        "trusted": true,
        "id": "9DKvSQQZdoW8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_train = dataset_creator(sroie_folder_path / 'train')\n",
        "dataset_test = dataset_creator(sroie_folder_path / 'test')"
      ],
      "metadata": {
        "trusted": true,
        "id": "4tlZbwRldoW8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### train-test split"
      ],
      "metadata": {
        "id": "AOeGBg0pdoW8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "random.seed(42)\n",
        "random.shuffle(dataset_test)\n",
        "dataset_val = dataset_test[174:]\n",
        "dataset_test = dataset_test[:174]\n",
        "print(len(dataset_val))\n",
        "print (len(dataset_test))"
      ],
      "metadata": {
        "trusted": true,
        "id": "ebt3dy9WdoW8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_train[0][0][\"x0\"]"
      ],
      "metadata": {
        "trusted": true,
        "id": "T5mOp58FdoW8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "from transformers import LayoutLMv3Tokenizer\n",
        "import torch\n",
        "\n",
        "class InvoiceDataset(Dataset):\n",
        "    def __init__(self, invoice_list, tokenizer, image_folder_path):\n",
        "        self.invoice_list = invoice_list\n",
        "        self.tokenizer = tokenizer\n",
        "        self.image_folder_path = image_folder_path\n",
        "        self.label_map = {\n",
        "            \"S-COMPANY\": 0,\n",
        "            \"S-ADDRESS\": 1,\n",
        "            \"S-DATE\": 2,\n",
        "            \"S-TOTAL\": 3,\n",
        "            \"O\": 4,  # For 'Other'\n",
        "        }\n",
        "        self.max_length = 512\n",
        "\n",
        "        # Modify the transform to resize to 224x224, which aligns with model expectations\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.Resize((224, 224)),  # Resize to 224x224\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
        "        ])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.invoice_list)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        invoice_data = self.invoice_list[idx]\n",
        "        word_df = invoice_data[0]\n",
        "        image_width = invoice_data[1]\n",
        "        image_height = invoice_data[2]\n",
        "        image_path = word_df[\"filename\"].iloc[0]\n",
        "\n",
        "        words = []\n",
        "        bboxes = []\n",
        "        labels = []\n",
        "\n",
        "        # Load and preprocess the image\n",
        "        image_path = f\"{self.image_folder_path}/{image_path}.jpg\"\n",
        "        try:\n",
        "            image = Image.open(image_path).convert(\"RGB\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading image {image_path}: {e}\")\n",
        "            return None\n",
        "\n",
        "        image = self.transform(image)  # Shape: [3, 224, 224]\n",
        "\n",
        "        # Add a batch dimension to the image tensor\n",
        "        image = image.unsqueeze(0)  # Now shape: [1, 3, 224, 224]\n",
        "\n",
        "        for _, word_data in word_df.iterrows():\n",
        "            word = word_data['line']\n",
        "            label = word_data['label']\n",
        "            bbox = [\n",
        "                word_data['x0'],\n",
        "                word_data['y0'],\n",
        "                word_data['x2'],\n",
        "                word_data['y2']\n",
        "            ]\n",
        "\n",
        "            # Normalize the bounding boxes\n",
        "            normalized_bbox = [\n",
        "                bbox[0] * 1000 / image_width,\n",
        "                bbox[1] * 1000 / image_height,\n",
        "                bbox[2] * 1000 / image_width,\n",
        "                bbox[3] * 1000 / image_height\n",
        "            ]\n",
        "\n",
        "            words.append(word)\n",
        "            bboxes.append(normalized_bbox)\n",
        "            labels.append(label)\n",
        "\n",
        "        # Tokenize the words with bounding boxes\n",
        "        tokens = self.tokenizer(\n",
        "            words,\n",
        "            boxes=bboxes,\n",
        "            truncation=True,\n",
        "            padding='max_length',\n",
        "            max_length=self.max_length,\n",
        "            is_split_into_words=True,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        # Convert labels to numerical format\n",
        "        labels = [self.label_map.get(label, self.label_map[\"O\"]) for label in word_df['label'].tolist()]\n",
        "\n",
        "        # Pad labels to max length with -100\n",
        "        padded_labels = labels + [-100] * (self.max_length - len(labels))  # Use -100 for padding\n",
        "        labels_tensor = torch.tensor(padded_labels, dtype=torch.long)\n",
        "\n",
        "        # Ensure the bbox tensor is correctly padded\n",
        "        bbox_tensor = tokens['bbox'].squeeze(0)\n",
        "        if bbox_tensor.size(0) < self.max_length:\n",
        "            padding = torch.zeros((self.max_length - bbox_tensor.size(0), 4), dtype=torch.float32)  # Pad with zeros\n",
        "            bbox_tensor = torch.cat([bbox_tensor, padding], dim=0)\n",
        "\n",
        "        # Convert everything to long\n",
        "        input_ids_tensor = tokens['input_ids'].squeeze(0).long()\n",
        "        attention_mask_tensor = tokens['attention_mask'].squeeze(0).long()\n",
        "\n",
        "        return  {\n",
        "            'input_ids': input_ids_tensor,\n",
        "            'attention_mask': attention_mask_tensor,\n",
        "            'bbox': bbox_tensor.to(torch.long),\n",
        "            'labels': labels_tensor,\n",
        "            'pixel_values': image.squeeze(0)  # Remove batch dimension for the final output shape [3, 224, 224]\n",
        "        }\n",
        "\n",
        "# Initialize tokenizer\n",
        "tokenizer = LayoutLMv3Tokenizer.from_pretrained(\"mp-02/layoutlmv3-large-cord2\")\n",
        "\n",
        "# Create your dataset\n",
        "image_folder_path = '/kaggle/input/sroie-datasetv2/SROIE2019/train/img'\n",
        "dataset = InvoiceDataset(dataset_train, tokenizer=tokenizer, image_folder_path=image_folder_path)\n",
        "val_set = InvoiceDataset(dataset_val, tokenizer=tokenizer, image_folder_path=\"/kaggle/input/sroie-datasetv2/SROIE2019/test/img\")\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "9H3dUnjydoW8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ensure Tensor Shapes\n",
        "It is crucial to verify that the shapes of all tensors are correct before proceeding with model training. This includes ensuring that input tensors match the expected dimensions of the model and that target tensors (labels) align with the input tensor shapes. Proper shape management helps prevent runtime errors and ensures the model learns effectively.\n"
      ],
      "metadata": {
        "id": "u4FOeq4SdoW8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset[0][\"input_ids\"].shape)\n",
        "print(dataset[0][\"attention_mask\"].shape)\n",
        "print(dataset[0][\"bbox\"].shape)\n",
        "print(dataset[0][\"labels\"].shape)\n",
        "print(dataset[0][\"pixel_values\"].shape)"
      ],
      "metadata": {
        "trusted": true,
        "id": "f45KIctidoW8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[0][\"bbox\"]"
      ],
      "metadata": {
        "trusted": true,
        "id": "08AnKrJHdoW9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "from transformers import logging as transformers_logging\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "transformers_logging.set_verbosity_error()\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "VzN2fxzDdoW9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "wandb.login(key=\"43683e6439b3f848199c0e333e5ffdc8c1695604\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "Gt7wCpnmdoW9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training\n",
        "\n",
        "In this section, we define the training parameters and initialize the `Trainer` for fine-tuning the LayoutLMv3 model on the SERIO dataset. The training arguments include evaluation strategies, logging settings, and learning rate specifications. The model is configured to freeze the first few layers to retain pre-trained weights while allowing the rest to be trainable. The `compute_metrics` function is utilized to evaluate the model's performance during training.\n",
        "\n",
        "The following code initializes the model and the training process.\n"
      ],
      "metadata": {
        "id": "mQ03oVPudoW9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "from transformers import LayoutLMv3ForTokenClassification, Trainer, TrainingArguments\n",
        "\n",
        "def compute_metrics(pred):\n",
        "    logits = pred.predictions\n",
        "    labels = pred.label_ids\n",
        "\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "\n",
        "    mask = labels != -100\n",
        "    labels = labels[mask]\n",
        "    predictions = predictions[mask]\n",
        "\n",
        "    accuracy = accuracy_score(labels, predictions)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='weighted')\n",
        "\n",
        "    return {\n",
        "        'accuracy': accuracy,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1': f1\n",
        "    }\n",
        "\n",
        "# Define class weights\n",
        "class_weights = torch.tensor([5.0, 5.0, 5.0, 5.0, 1.0])  # Adjusted weights\n",
        "\n",
        "model = LayoutLMv3ForTokenClassification.from_pretrained(\n",
        "    \"mp-02/layoutlmv3-large-cord2\",\n",
        "    num_labels=5,\n",
        "    hidden_dropout_prob=0.2\n",
        ")\n",
        "\n",
        "for idx, param in enumerate(model.parameters()):\n",
        "    param.requires_grad = idx >= 8\n",
        "\n",
        "def custom_loss_func(logits, labels):\n",
        "    loss_fct = torch.nn.CrossEntropyLoss(weight=class_weights)  # Use class weights here\n",
        "    return loss_fct(logits.view(-1, model.config.num_labels), labels.view(-1))\n",
        "\n",
        "model.loss_fct = custom_loss_func\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    evaluation_strategy='epoch',\n",
        "    save_strategy='epoch',\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=100,\n",
        "    num_train_epochs=40,\n",
        "    learning_rate=1e-5,\n",
        "    report_to='wandb',\n",
        "    run_name='layoutlmv3-training',\n",
        "    per_device_train_batch_size=2,\n",
        "    per_device_eval_batch_size=2,\n",
        "    save_total_limit=1,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model='f1',\n",
        "    greater_is_better=True\n",
        ")\n",
        "\n",
        "# Initialize the Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=dataset,  # Use the entire training dataset\n",
        "    eval_dataset=val_set,    # Use the entire validation dataset\n",
        "    compute_metrics=compute_metrics\n",
        ")"
      ],
      "metadata": {
        "trusted": true,
        "id": "a-3S3duQdoW9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "_kg_hide-output": true,
        "trusted": true,
        "id": "oEhjfU9KdoW9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Best Model Selection\n",
        "\n",
        "The best model is the one with the highest F1 score on the validation set. This metric provides a balance between precision and recall, making it a suitable choice for evaluating model performance in tasks where class distribution may be imbalanced.\n"
      ],
      "metadata": {
        "id": "I9--N7K0doW9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best_model = LayoutLMv3ForTokenClassification.from_pretrained('/kaggle/working/results/checkpoint-6573')"
      ],
      "metadata": {
        "trusted": true,
        "id": "9vCBTE2BdoW9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(\n",
        "    model=best_model,\n",
        "    args=training_args,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "training_eval = trainer.evaluate(eval_dataset=dataset)\n",
        "print(\"Training Evaluation:\", training_eval)\n",
        "\n",
        "val_eval = trainer.evaluate(eval_dataset=val_set)\n",
        "print(\"Validation Evaluation:\", val_eval)\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "sSrCNP9NdoW9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluating Model performance on the Test Set\n",
        "\n",
        "In this section, we will evaluate the performance of our trained model on the test set."
      ],
      "metadata": {
        "id": "4mbakircdoW9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_set = InvoiceDataset(dataset_test, tokenizer=tokenizer, image_folder_path=\"/kaggle/input/sroie-datasetv2/SROIE2019/test/img\")\n",
        "test_evaluation = trainer.evaluate(eval_dataset= test_set)\n",
        "print(test_evaluation)"
      ],
      "metadata": {
        "trusted": true,
        "id": "580Mb7ykdoW-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Great Achievement!\n",
        "We have achieved an impressive F1 score of **95.8%**! ðŸŽ‰"
      ],
      "metadata": {
        "id": "w0R6_jTXdoW-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Output Production Phase\n",
        "\n",
        "In this phase of the pipeline, we aim to produce the output in JSON file format. This is crucial for integrating the model's predictions with other applications or systems that require structured data.\n",
        "\n",
        "### Steps to Produce JSON Output:\n",
        "\n",
        "1. **Extract Predictions**: After evaluating the model on the validation/test dataset, extract the relevant predictions (e.g., bounding boxes, labels, company name, date, address, and total).\n",
        "  \n",
        "2. **Structure the Data**: Organize the extracted data into a dictionary format. Each entry should correspond to a specific field that we want to include in the JSON output.\n",
        "\n",
        "3. **Convert to JSON**: Use Python's built-in `json` module to convert the structured data into JSON format.\n",
        "\n",
        "4. **Save the JSON File**: Write the JSON data to a file for further use or analysis.\n",
        "\n",
        "This phase is typically referred to as the **Output Generation Phase** in the machine learning pipeline, where we focus on converting model predictions into a consumable format.\n"
      ],
      "metadata": {
        "id": "R1XT_rDodoW-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### I will use on observation from test set to extract the output as json file a"
      ],
      "metadata": {
        "id": "souOsbPsdoW-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_labels(sample)\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    observation = {k: v for k, v in sample.items() if k != 'labels'}\n",
        "\n",
        "    input_ids = observation['input_ids'].unsqueeze(0)\n",
        "    attention_mask = observation['attention_mask'].unsqueeze(0)\n",
        "    bbox = observation['bbox'].unsqueeze(0)\n",
        "    pixel_values = observation['pixel_values'].unsqueeze(0)\n",
        "\n",
        "    best_model = best_model.to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = best_model(input_ids=input_ids,\n",
        "                        attention_mask=attention_mask,\n",
        "                        bbox=bbox,\n",
        "                        pixel_values=pixel_values)\n",
        "    return outputs\n",
        "\n",
        "outputs = generate_labels(test_set[0])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-21T00:10:09.438402Z",
          "iopub.execute_input": "2024-10-21T00:10:09.438824Z",
          "iopub.status.idle": "2024-10-21T00:10:09.447803Z",
          "shell.execute_reply.started": "2024-10-21T00:10:09.438782Z",
          "shell.execute_reply": "2024-10-21T00:10:09.446958Z"
        },
        "trusted": true,
        "id": "v-0QVyozdoW-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outputs"
      ],
      "metadata": {
        "trusted": true,
        "id": "Gs3mtl1LdoW-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample = dataset_test[0][0].drop([\"label\"], axis= 1)"
      ],
      "metadata": {
        "trusted": true,
        "id": "mLeBlMvJdoW-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_test[0][0]['label']"
      ],
      "metadata": {
        "trusted": true,
        "id": "qQhBlj3LdoW-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "\n",
        "def logits_to_labels(outputs, label_map: dict) -> pd.Series:\n",
        "    \"\"\"\n",
        "    Convert logits from TokenClassifierOutput to labels based on the label mapping.\n",
        "\n",
        "    Parameters:\n",
        "        outputs (TokenClassifierOutput): The output from the model containing logits.\n",
        "        label_map (dict): A mapping from label names to indices.\n",
        "\n",
        "    Returns:\n",
        "        pd.Series: A Pandas Series containing the predicted labels.\n",
        "    \"\"\"\n",
        "    # Extract logits from the outputs\n",
        "    logits = outputs.logits  # Access the logits attribute\n",
        "\n",
        "    # Get the predicted indices from the logits\n",
        "    predicted_indices = torch.argmax(logits, dim=-1)\n",
        "\n",
        "    # Create a reverse mapping from indices to labels\n",
        "    index_to_label = {v: k for k, v in label_map.items()}\n",
        "\n",
        "    # Map predicted indices to labels\n",
        "    # Use squeeze to remove unnecessary dimensions (batch size = 1 assumed)\n",
        "    predicted_labels = [index_to_label[idx.item()] for idx in predicted_indices.squeeze()]\n",
        "\n",
        "    # Create a Pandas Series from the predicted labels\n",
        "    labels_series = pd.Series(predicted_labels)\n",
        "\n",
        "    return labels_series\n",
        "\n",
        "# Example label mapping\n",
        "label_map = {\n",
        "    \"S-COMPANY\": 0,\n",
        "    \"S-ADDRESS\": 1,\n",
        "    \"S-DATE\": 2,\n",
        "    \"S-TOTAL\": 3,\n",
        "    \"O\": 4,  # For 'Other'\n",
        "}\n",
        "\n",
        "# Assuming 'outputs' is your TokenClassifierOutput object\n",
        "# Convert logits to labels\n",
        "labels_series = logits_to_labels(outputs, label_map)\n",
        "print(labels_series)\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "yFK4PYFHdoW_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(sample)"
      ],
      "metadata": {
        "trusted": true,
        "id": "Cgj6cJ7xdoW_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_output =pd.concat([sample, labels_series[:len(sample)]], axis= 1)"
      ],
      "metadata": {
        "trusted": true,
        "id": "l2gkXM_ldoW_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_output.columns = ['filename', 'x0', 'y0', 'x2', 'y2', 'line', 'label']  # Rename columns as needed\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "GzsWMzv9doW_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_output"
      ],
      "metadata": {
        "trusted": true,
        "id": "zrOXeZ4udoW_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from collections import Counter  # Ensure to import Counter\n",
        "\n",
        "def reverse_words_and_vote(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    # Group by bounding box coordinates\n",
        "    grouped = df.groupby(['x0', 'y0', 'x2', 'y2'])\n",
        "\n",
        "    final_labels = []\n",
        "\n",
        "    for (x0, y0, x2, y2), group in grouped:\n",
        "        # Reverse the words in the same bounding box\n",
        "        reversed_words = ' '.join(reversed(group['line'].tolist()))\n",
        "\n",
        "        # Count the occurrences of each label\n",
        "        label_counts = Counter(group['label'])\n",
        "\n",
        "        # Get the most common label\n",
        "        most_common_label, count = label_counts.most_common(1)[0]\n",
        "\n",
        "        # Add the reversed words along with bounding box coordinates and label to the final output\n",
        "        final_labels.append({\n",
        "            'filename': group['filename'].iloc[0],\n",
        "            'x0': x0,\n",
        "            'y0': y0,\n",
        "            'x2': x2,\n",
        "            'y2': y2,\n",
        "            'line': reversed_words,\n",
        "            'label': most_common_label,\n",
        "            'count': count\n",
        "        })\n",
        "\n",
        "    # Create a new DataFrame with the final results\n",
        "    final_df = pd.DataFrame(final_labels)\n",
        "\n",
        "    return final_df\n",
        "\n",
        "# Example usage\n",
        "# Assuming `sample_output` is your existing DataFrame\n",
        "# sample_output = pd.DataFrame(...)  # Your DataFrame goes here\n",
        "new_sample_output = reverse_words_and_vote(sample_output)\n",
        "\n",
        "# Display the new DataFrame\n",
        "print(new_sample_output.drop(\"count\", axis=1))\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "aUW6k_MhdoW_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_sample_output.drop([\"count\",\"filename\"], axis= 1).to_json(f\"/kaggle/working/sample_output.json\", orient='records', lines=True)"
      ],
      "metadata": {
        "trusted": true,
        "id": "-XnFHtCzdoW_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Path to your JSON file\n",
        "json_file_path = '/kaggle/working/sample_output.json'  # Update with your actual JSON file path\n",
        "\n",
        "# Initialize an empty list to store the loaded JSON data\n",
        "data = []\n",
        "\n",
        "try:\n",
        "    # Open and read the JSON file\n",
        "    with open(json_file_path, 'r') as file:\n",
        "        # Check if the file is line-delimited JSON (each line is a separate JSON object)\n",
        "        for line in file:\n",
        "            try:\n",
        "                # Load each line as a separate JSON object\n",
        "                data.append(json.loads(line))\n",
        "            except json.JSONDecodeError as e:\n",
        "                print(f\"Error decoding JSON on line: {line.strip()} - {e}\")\n",
        "\n",
        "    # If the file is not line-delimited, you can load it as a whole\n",
        "    # Uncomment this section if you expect the entire file to be a single JSON object or array\n",
        "    # data = json.load(file)\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"File not found: {json_file_path}\")\n",
        "except json.JSONDecodeError as e:\n",
        "    print(f\"Error decoding JSON: {e}\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")\n",
        "\n",
        "# Print the loaded data in a readable JSON format\n",
        "try:\n",
        "    print(json.dumps(data, indent=4))  # Pretty print the JSON data\n",
        "except Exception as e:\n",
        "    print(f\"Error printing JSON data: {e}\")\n"
      ],
      "metadata": {
        "_kg_hide-input": true,
        "_kg_hide-output": true,
        "trusted": true,
        "id": "S2zMI1AidoW_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "from PIL import Image, ImageDraw\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "label_colors = {\n",
        "    \"S-COMPANY\": \"blue\",\n",
        "    \"S-ADDRESS\": \"red\",\n",
        "    \"S-DATE\": \"green\",\n",
        "    \"S-TOTAL\": \"orange\",\n",
        "    \"O\": \"black\"\n",
        "}\n",
        "\n",
        "def draw_bounding_boxes(image_path, df):\n",
        "    try:\n",
        "        image = Image.open(image_path)\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Image file not found: {image_path}\")\n",
        "        return None\n",
        "\n",
        "    draw = ImageDraw.Draw(image)\n",
        "\n",
        "    for index, row in df.iterrows():\n",
        "        x0, y0, x2, y2, label = row['x0'], row['y0'], row['x2'], row['y2'], row['label']\n",
        "\n",
        "        color = label_colors.get(label, \"black\")\n",
        "\n",
        "        draw.rectangle([x0, y0, x2, y2], outline=color, width=2)\n",
        "\n",
        "    return image\n",
        "\n",
        "image_filename = sample_output['filename'].iloc[0]\n",
        "image_path = f\"/kaggle/input/sroie-datasetv2/SROIE2019/test/img/{image_filename}.jpg\"\n",
        "\n",
        "output_image = draw_bounding_boxes(image_path, new_sample_output)\n",
        "\n",
        "if output_image is not None:\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    plt.imshow(output_image)\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"No image to display.\")\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "ATrR8hNidoW_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install huggingface_hub\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-20T23:53:06.436026Z",
          "iopub.execute_input": "2024-10-20T23:53:06.436537Z",
          "iopub.status.idle": "2024-10-20T23:53:17.77878Z",
          "shell.execute_reply.started": "2024-10-20T23:53:06.436498Z",
          "shell.execute_reply": "2024-10-20T23:53:17.777736Z"
        },
        "trusted": true,
        "id": "ajkD4UBXdoXA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Push Our Model to Hugging Face\n",
        "\n",
        "In this section, we will upload our fine-tuned model to the Hugging Face Model Hub. This allows others to easily access and use our model."
      ],
      "metadata": {
        "id": "azetVDImdoXA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "login()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-21T00:03:07.39619Z",
          "iopub.execute_input": "2024-10-21T00:03:07.397184Z",
          "iopub.status.idle": "2024-10-21T00:03:07.420198Z",
          "shell.execute_reply.started": "2024-10-21T00:03:07.39712Z",
          "shell.execute_reply": "2024-10-21T00:03:07.419548Z"
        },
        "trusted": true,
        "id": "RLbF6Ms2doXA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from transformers import AutoModelForTokenClassification, AutoTokenizer\n",
        "from huggingface_hub import HfApi, HfFolder\n",
        "\n",
        "# Step 1: Load Your Model Checkpoint\n",
        "checkpoint_path = '/kaggle/working/results/checkpoint-6573'  # Update with your checkpoint path\n",
        "model = AutoModelForTokenClassification.from_pretrained(checkpoint_path)\n",
        "\n",
        "\n",
        "\n",
        "model_name = \"MohmaedElnamir/fine-tuned-layoutlmv3-sroie\"  # Update with your Hugging Face username and desired model name\n",
        "\n",
        "\n",
        "model.push_to_hub(model_name)\n",
        "\n",
        "\n",
        "print(f\"Model uploaded to Hugging Face: {model_name}\")\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-21T00:03:28.509491Z",
          "iopub.execute_input": "2024-10-21T00:03:28.509882Z",
          "iopub.status.idle": "2024-10-21T00:04:42.163619Z",
          "shell.execute_reply.started": "2024-10-21T00:03:28.509842Z",
          "shell.execute_reply": "2024-10-21T00:04:42.163011Z"
        },
        "trusted": true,
        "id": "TlwBMpyjdoXA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer_name = \"mp-02/layoutlmv3-large-cord2\"  # Replace with the original model name you used\n",
        "tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
        "model.push_to_hub(model_name)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-21T00:05:59.749859Z",
          "iopub.execute_input": "2024-10-21T00:05:59.750544Z",
          "iopub.status.idle": "2024-10-21T00:06:07.732692Z",
          "shell.execute_reply.started": "2024-10-21T00:05:59.750505Z",
          "shell.execute_reply": "2024-10-21T00:06:07.731941Z"
        },
        "trusted": true,
        "id": "y4I8JEoxdoXA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "etf72uiddoXA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}